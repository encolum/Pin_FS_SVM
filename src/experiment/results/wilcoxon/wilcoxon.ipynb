{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c36d7e8",
   "metadata": {},
   "source": [
    "### Prepare the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1fe6ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Merging : cleveland ---\n",
      "\n",
      "Processing model: PinFSSVM\n",
      "  Loading: cleveland/original/PinFSSVM_auc_folds.xlsx\n",
      "  Loading: cleveland/noise/PinFSSVM_auc_folds.xlsx\n",
      "  Loading: cleveland/outlier/PinFSSVM_auc_folds.xlsx\n",
      "  Loading: cleveland/both/PinFSSVM_auc_folds.xlsx\n",
      "  Total samples for PinFSSVM: 40\n",
      "\n",
      "Processing model: MILP1\n",
      "  Loading: cleveland/original/MILP1_auc_folds.xlsx\n",
      "  Loading: cleveland/noise/MILP1_auc_folds.xlsx\n",
      "  Loading: cleveland/outlier/MILP1_auc_folds.xlsx\n",
      "  Loading: cleveland/both/MILP1_auc_folds.xlsx\n",
      "  Total samples for MILP1: 40\n",
      "\n",
      "Processing model: RFESVM\n",
      "  Loading: cleveland/original/RFESVM_auc_folds.xlsx\n",
      "  Loading: cleveland/noise/RFESVM_auc_folds.xlsx\n",
      "  Loading: cleveland/outlier/RFESVM_auc_folds.xlsx\n",
      "  Loading: cleveland/both/RFESVM_auc_folds.xlsx\n",
      "  Total samples for RFESVM: 40\n",
      "\n",
      "Processing model: FisherSVM\n",
      "  Loading: cleveland/original/FisherSVM_auc_folds.xlsx\n",
      "  Loading: cleveland/noise/FisherSVM_auc_folds.xlsx\n",
      "  Loading: cleveland/outlier/FisherSVM_auc_folds.xlsx\n",
      "  Loading: cleveland/both/FisherSVM_auc_folds.xlsx\n",
      "  Total samples for FisherSVM: 40\n",
      "\n",
      "Processing model: L1SVM\n",
      "  Loading: cleveland/original/L1SVM_auc_folds.xlsx\n",
      "  Loading: cleveland/noise/L1SVM_auc_folds.xlsx\n",
      "  Loading: cleveland/outlier/L1SVM_auc_folds.xlsx\n",
      "  Loading: cleveland/both/L1SVM_auc_folds.xlsx\n",
      "  Total samples for L1SVM: 40\n",
      "\n",
      "--- Kết quả merge cho dataset cleveland ---\n",
      "Total DataFrame shape: (200, 10)\n",
      "Total missing files: 0\n",
      " Saved to: d:\\Optimal-Robust-Feature-Selection-For-Support-Vector-Machines-With-Pinball-Loss\\src\\experiment\\results\\wilcoxon\\cleveland\\merged_cleveland_data.xlsx\n",
      "   Shape: (200, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "current_dir = os.getcwd()\n",
    "BASE_DIR = current_dir\n",
    "\n",
    "ALL_DATASETS = ['wdbc', 'sonar', 'ionosphere', 'diabetes', 'cleveland', 'colon']\n",
    "DATASET_TYPES = ['original', 'noise', 'outlier', 'both'] \n",
    "ALL_MODELS = ['RFESVM', 'PinFSSVM', 'PinballSVM', 'MILP1', 'L2SVM', 'L1SVM', 'FisherSVM']\n",
    "\n",
    "\n",
    "TARGET_DATASET = 'cleveland' \n",
    "MODELS_TO_COMPARE = ['PinFSSVM', 'MILP1', 'RFESVM', 'FisherSVM', 'L1SVM']\n",
    "\n",
    "def load_excel_file(file_path):\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"  Warning: File not found: {file_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def merge_single_dataset(dataset_name, models_list):\n",
    "    \n",
    "    all_data = []\n",
    "    missing_files = []\n",
    "    \n",
    "    print(f\"\\n--- Merging : {dataset_name} ---\")\n",
    "    \n",
    "    for model in models_list:\n",
    "        print(f\"\\nProcessing model: {model}\")\n",
    "        model_data = []\n",
    "        \n",
    "        for dataset_type in DATASET_TYPES:\n",
    "            file_path = os.path.join(BASE_DIR, dataset_name, dataset_type, f\"{model}_auc_folds.xlsx\")\n",
    "            \n",
    "            print(f\"  Loading: {dataset_name}/{dataset_type}/{model}_auc_folds.xlsx\")\n",
    "            \n",
    "            df = load_excel_file(file_path)\n",
    "            \n",
    "            if df is not None:\n",
    "                df['Model'] = model\n",
    "                df['Dataset'] = dataset_name\n",
    "                df['Dataset_Type'] = dataset_type\n",
    "                \n",
    "                model_data.append(df)\n",
    "                all_data.append(df)\n",
    "            else:\n",
    "                missing_files.append(file_path)\n",
    "        \n",
    "        total_samples_for_model = sum([len(df) for df in model_data])\n",
    "        print(f\"  Total samples for {model}: {total_samples_for_model}\")\n",
    "    \n",
    "    if all_data:\n",
    "        merged_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\n--- Kết quả merge cho dataset {dataset_name} ---\")\n",
    "        print(f\"Total DataFrame shape: {merged_df.shape}\")\n",
    "        print(f\"Total missing files: {len(missing_files)}\")\n",
    "        \n",
    "        return merged_df, missing_files\n",
    "    else:\n",
    "        print(\"No data to merge!\")\n",
    "        return None, missing_files\n",
    "\n",
    "merged_data, missing_files = merge_single_dataset(TARGET_DATASET, MODELS_TO_COMPARE)\n",
    "\n",
    "\n",
    "if merged_data is not None:\n",
    "    \n",
    "    output_file = os.path.join(BASE_DIR, TARGET_DATASET,f\"merged_{TARGET_DATASET}_data.xlsx\")\n",
    "    \n",
    "    try:\n",
    "        merged_data.to_excel(output_file, index=False)\n",
    "        print(f\" Saved to: {output_file}\")\n",
    "        print(f\"   Shape: {merged_data.shape}\")\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "\n",
    "else:\n",
    "    print(\"No data to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a9cc042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha for testing: 0.05\n",
      "\n",
      "=== WILCOXON SIGNED-RANK TEST for dataset cleveland ===\n",
      "======================================================================\n",
      "\n",
      "PinFSSVM vs MILP1\n",
      "----------------------------------------\n",
      "Mean Difference: 0.0017\n",
      "95% CI: [0.0000, 0.0000]\n",
      "P-value (two-sided): 0.7323\n",
      "P-value (PinFSSVM > MILP1): 0.3661\n",
      "Conclusion: No significant difference\n",
      "\n",
      "PinFSSVM vs RFESVM\n",
      "----------------------------------------\n",
      "Mean Difference: 0.0023\n",
      "95% CI: [0.0000, 0.0000]\n",
      "P-value (two-sided): 0.6389\n",
      "P-value (PinFSSVM > RFESVM): 0.3194\n",
      "Conclusion: No significant difference\n",
      "\n",
      "PinFSSVM vs FisherSVM\n",
      "----------------------------------------\n",
      "Mean Difference: 0.0051\n",
      "95% CI: [0.0000, 0.0085]\n",
      "P-value (two-sided): 0.4202\n",
      "P-value (PinFSSVM > FisherSVM): 0.2101\n",
      "Conclusion: No significant difference\n",
      "\n",
      "PinFSSVM vs L1SVM\n",
      "----------------------------------------\n",
      "Mean Difference: 0.0078\n",
      "95% CI: [0.0000, 0.0132]\n",
      "P-value (two-sided): 0.0403\n",
      "P-value (PinFSSVM > L1SVM): 0.0201\n",
      "Conclusion: PinFSSVM significantly better\n",
      "\n",
      "Combined statistical summary saved to: d:\\Optimal-Robust-Feature-Selection-For-Support-Vector-Machines-With-Pinball-Loss\\src\\experiment\\results\\wilcoxon\\cleveland\\statistical_summary_combined_cleveland.xlsx\n",
      "\n",
      "Combined Results Shape: (4, 11)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "=== SUMMARY OF COMBINED STATISTICAL TESTS (cleveland) ===\n",
      "      Comparison       Dataset   Alpha  P_value_2sided  P_value_1sided  CI_Lower  CI_Upper  Mean_Difference           Conclusion           Benjamini-Hochberg_Corrected_P_1sided_Greater Significant_Benjamini-Hochberg (Main > Baseline)\n",
      "PinFSSVM vs FisherSVM cleveland  0.05       0.4202          0.2101        0.0      0.0085       0.0051          No significant difference                     0.3661                                          False                      \n",
      "    PinFSSVM vs L1SVM cleveland  0.05       0.0403          0.0201        0.0      0.0132       0.0078      PinFSSVM significantly better                     0.0805                                          False                      \n",
      "    PinFSSVM vs MILP1 cleveland  0.05       0.7323          0.3661        0.0      0.0000       0.0017          No significant difference                     0.3661                                          False                      \n",
      "   PinFSSVM vs RFESVM cleveland  0.05       0.6389          0.3194        0.0      0.0000       0.0023          No significant difference                     0.3661                                          False                      \n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon,bootstrap\n",
    "import scipy.stats as stats\n",
    "n_resamples = 10000\n",
    "confidence_level = 0.95\n",
    "alpha = 0.05\n",
    "print(f'Alpha for testing: {alpha}')\n",
    "\n",
    "def get_bootstrap_ci_median_diff(data1, data2, n_resamples, confidence_level):\n",
    "    differences = np.array(data1) - np.array(data2)\n",
    "    if len(differences) < 2: return (np.nan, np.nan)\n",
    "    try:\n",
    "        res = bootstrap((differences,), np.median, confidence_level=confidence_level,\n",
    "                        n_resamples=n_resamples, method='percentile', random_state=42)\n",
    "        return (res.confidence_interval.low, res.confidence_interval.high)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error in bootstrap CI (median diff): {e}\")\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "results_log = []\n",
    "\n",
    "print(f\"\\n=== WILCOXON SIGNED-RANK TEST for dataset {TARGET_DATASET} ===\")\n",
    "print('='*70)\n",
    "\n",
    "for i in range(len(MODELS_TO_COMPARE)-1):\n",
    "    model1 = MODELS_TO_COMPARE[0]  \n",
    "    model2 = MODELS_TO_COMPARE[i+1]  \n",
    "    \n",
    "    model1_data = merged_data[merged_data['Model'] == model1].copy()\n",
    "    model2_data = merged_data[merged_data['Model'] == model2].copy()\n",
    "    \n",
    "    print(f\"\\n{model1} vs {model2}\")\n",
    "    print('-' * 40)\n",
    "    \n",
    "    # Make comparison DataFrame \n",
    "    comparison_df = pd.merge(\n",
    "        model1_data[['Dataset_Type', 'Fold', 'AUC']],\n",
    "        model2_data[['Dataset_Type', 'Fold', 'AUC']],\n",
    "        on=['Dataset_Type', 'Fold'],\n",
    "        suffixes=(f'_{model1}', f'_{model2}')\n",
    "    )\n",
    "    \n",
    "    if len(comparison_df) > 0:\n",
    "        auc1_col = f'AUC_{model1}'\n",
    "        auc2_col = f'AUC_{model2}'\n",
    "        \n",
    "        # Prepare for Wilcoxon test\n",
    "        model1_scores = comparison_df[auc1_col].values\n",
    "        model2_scores = comparison_df[auc2_col].values\n",
    "        mean_diff = np.mean(model1_scores - model2_scores)\n",
    "        ci_lower, ci_upper = get_bootstrap_ci_median_diff(model1_scores, model2_scores, n_resamples = n_resamples, confidence_level=confidence_level)\n",
    "        \n",
    "        # Wilcoxon test\n",
    "        statistic, p_value = wilcoxon(model1_scores, model2_scores, alternative='two-sided')\n",
    "        statistic_greater, p_value_greater = wilcoxon(model1_scores, model2_scores, alternative='greater')\n",
    "        \n",
    "        if p_value_greater < alpha:\n",
    "            conclusion = f\"{model1} significantly better\"\n",
    "        else:\n",
    "            statistic_less, p_value_less = wilcoxon(model1_scores, model2_scores, alternative='less')\n",
    "            if p_value_less < alpha:\n",
    "                conclusion = f\"{model2} significantly better\"\n",
    "            else:\n",
    "                conclusion = \"No significant difference\"\n",
    "        \n",
    "        result_row = {\n",
    "            'Comparison': f\"{model1} vs {model2}\",\n",
    "            'Dataset': TARGET_DATASET,\n",
    "            'Alpha': alpha,\n",
    "            'P_value_2sided': p_value,\n",
    "            'P_value_1sided': p_value_greater,\n",
    "            'CI_Lower': ci_lower,\n",
    "            'CI_Upper': ci_upper,\n",
    "            'Mean_Difference': mean_diff,\n",
    "            'Conclusion': conclusion\n",
    "        }\n",
    "        results_log.append(result_row)\n",
    "        \n",
    "        # Compact output\n",
    "        print(f\"Mean Difference: {mean_diff:.4f}\")\n",
    "        print(f\"95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "        print(f\"P-value (two-sided): {p_value:.4f}\")\n",
    "        print(f\"P-value ({model1} > {model2}): {p_value_greater:.4f}\")\n",
    "        print(f\"Conclusion: {conclusion}\")\n",
    "        \n",
    "        # Save comparison results to Excel\n",
    "        comparison_df['Difference'] = comparison_df[auc1_col] - comparison_df[auc2_col]\n",
    "        comparison_output = os.path.join(BASE_DIR, TARGET_DATASET, f\"comparison_{model1}_vs_{model2}_{TARGET_DATASET}.xlsx\")\n",
    "        comparison_df.to_excel(comparison_output, index=False)\n",
    "        \n",
    "    else:\n",
    "        print(\"No matching data for comparison\")\n",
    "        result_row = {\n",
    "            'Comparison': f\"{model1} vs {model2}\",\n",
    "            'Dataset': TARGET_DATASET,\n",
    "            'Alpha': alpha,\n",
    "            'P_value_2sided': np.nan,\n",
    "            'P_value_1sided': np.nan,\n",
    "            'CI_Lower': np.nan,\n",
    "            'CI_Upper': np.nan,\n",
    "            'Mean_Difference': np.nan,\n",
    "            'Conclusion': \"No matching data\"\n",
    "        }\n",
    "        results_log.append(result_row)\n",
    "\n",
    "if results_log:\n",
    "    results_df = pd.DataFrame(results_log)\n",
    "    results_df = results_df.sort_values(by=['Comparison'])\n",
    "    \n",
    "    valid_p_indices = results_df['P_value_1sided'].notna()\n",
    "    raw_p_values = results_df.loc[valid_p_indices, 'P_value_1sided'].tolist()\n",
    "\n",
    "    if raw_p_values:\n",
    "        from statsmodels.stats.multitest import multipletests\n",
    "        try:\n",
    "            reject_holm, p_corrected_holm, _, _ = multipletests(raw_p_values, alpha=alpha, method='fdr_bh')\n",
    "            results_df.loc[valid_p_indices, 'Benjamini-Hochberg_Corrected_P_1sided_Greater'] = p_corrected_holm\n",
    "            results_df.loc[valid_p_indices, 'Significant_Benjamini-Hochberg (Main > Baseline)'] = reject_holm\n",
    "        except Exception as e:\n",
    "            print(f\"Error during multiple comparison adjustment: {e}\")\n",
    "            results_df['Benjamini-Hochberg_Corrected_P_1sided_Greater'] = np.nan\n",
    "            results_df['Benjamini-Hochberg (Main > Baseline)'] = False\n",
    "            \n",
    "    output_filename = f\"statistical_summary_combined_{TARGET_DATASET}.xlsx\"\n",
    "    results_output_path = os.path.join(BASE_DIR, TARGET_DATASET, output_filename)\n",
    "    try:\n",
    "        results_df.to_excel(results_output_path, index=False)\n",
    "        print(f\"\\nCombined statistical summary saved to: {results_output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving combined summary: {e}\")\n",
    "\n",
    "    print(f\"\\nCombined Results Shape: {results_df.shape}\")\n",
    "    print('\\n' + '='*70)\n",
    "    print(f\"\\n=== SUMMARY OF COMBINED STATISTICAL TESTS ({TARGET_DATASET}) ===\")\n",
    "    pd.set_option('display.max_rows', None); pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 2000); pd.set_option('display.colheader_justify', 'center')\n",
    "    pd.set_option('display.precision', 4)\n",
    "    print(results_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo results were logged for combined statistical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6fd58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
