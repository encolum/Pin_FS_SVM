{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b17feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# Get the project root directory\n",
    "PATH = os.getcwd()  \n",
    "while not os.path.basename(PATH).startswith('Optimal-Robust-Feature-Selection'):\n",
    "    parent = os.path.dirname(PATH)\n",
    "    if parent == PATH:  # Reached filesystem root\n",
    "        break\n",
    "    PATH = parent\n",
    "\n",
    "FILE_PATH = os.path.join(PATH, 'Dataset', 'Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c000e8b",
   "metadata": {},
   "source": [
    "# Wdbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd88f5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing: original\n",
      "========================================\n",
      "Top 10 features with highest correlation to Target:\n",
      "Feature_28: 0.7936\n",
      "Feature_23: 0.7829\n",
      "Feature_8: 0.7766\n",
      "Feature_21: 0.7765\n",
      "Feature_3: 0.7426\n",
      "Feature_24: 0.7338\n",
      "Feature_1: 0.7300\n",
      "Feature_4: 0.7090\n",
      "Feature_7: 0.6964\n",
      "Feature_27: 0.6596\n",
      "\n",
      "========================================\n",
      "Processing: noise\n",
      "========================================\n",
      "Top 10 features with highest correlation to Target:\n",
      "Feature_24: 0.6201\n",
      "Feature_3: 0.6029\n",
      "Feature_4: 0.5703\n",
      "Feature_21: 0.5586\n",
      "Feature_23: 0.5522\n",
      "Feature_14: 0.5415\n",
      "Feature_1: 0.5251\n",
      "Feature_13: 0.4965\n",
      "Feature_27: 0.4739\n",
      "Feature_11: 0.4162\n",
      "\n",
      "========================================\n",
      "Processing: outlier\n",
      "========================================\n",
      "Top 10 features with highest correlation to Target:\n",
      "Feature_28: 0.6397\n",
      "Feature_8: 0.5837\n",
      "Feature_27: 0.5545\n",
      "Feature_23: 0.5409\n",
      "Feature_21: 0.5344\n",
      "Feature_7: 0.5298\n",
      "Feature_3: 0.5298\n",
      "Feature_1: 0.5186\n",
      "Feature_26: 0.5120\n",
      "Feature_6: 0.4783\n",
      "\n",
      "========================================\n",
      "Processing: noise_outlier\n",
      "========================================\n",
      "Top 10 features with highest correlation to Target:\n",
      "Feature_23: 0.4545\n",
      "Feature_4: 0.4429\n",
      "Feature_21: 0.4366\n",
      "Feature_3: 0.4296\n",
      "Feature_24: 0.4269\n",
      "Feature_1: 0.3945\n",
      "Feature_27: 0.3262\n",
      "Feature_26: 0.2901\n",
      "Feature_22: 0.2900\n",
      "Feature_13: 0.2863\n"
     ]
    }
   ],
   "source": [
    "def process_wdbc_dataset(file_path, dataset_name):\n",
    "    print(f\"\\n{'='*40}\\nProcessing: {dataset_name}\\n{'='*40}\")\n",
    "    \n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "    X = df.iloc[:, 1:].copy()\n",
    "    X = X.rename(columns={1: 'Target'})\n",
    "    X['Target'] = np.where(X['Target'] == 'M', 1, -1)\n",
    "\n",
    "    new_columns = {'Target': 'Target'}\n",
    "    for col in X.columns:\n",
    "        if col != 'Target':\n",
    "            new_columns[col] = f'Feature_{col - 1}'\n",
    "    X = X.rename(columns=new_columns)\n",
    "\n",
    "    corr_matrix = X.corr()\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='YlGnBu')\n",
    "    plt.title(f'Correlation Matrix - {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    save_dir = os.path.join(PATH, 'src', 'experiment', 'results', 'correlation_matrix', 'wdbc')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(os.path.join(save_dir, f'correlation_heatmap_wdbc_{dataset_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    target_correlations = corr_matrix['Target'].abs().sort_values(ascending=False)\n",
    "\n",
    "    # Lấy 10 đặc trưng ảnh hưởng lớn nhất (bỏ 'Target')\n",
    "    top_10_features = target_correlations[1:11]\n",
    "\n",
    "    print(\"Top 10 features with highest correlation to Target:\")\n",
    "    for feature, corr in top_10_features.items():\n",
    "        print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "\n",
    "# Danh sách các file và tên tương ứng\n",
    "datasets = {\n",
    "    \"original\": os.path.join(FILE_PATH, \"wdbc.data.txt\"),\n",
    "    \"noise\": os.path.join(FILE_PATH, \"wdbc_noisy_label_feature.txt\"),\n",
    "    \"outlier\": os.path.join(FILE_PATH, \"wdbc_noisy_label_outlier.txt\"),\n",
    "    \"noise_outlier\": os.path.join(FILE_PATH, \"wdbc_both_noise_outlier.txt\")\n",
    "}\n",
    "\n",
    "for name, file_path in datasets.items():\n",
    "    if os.path.exists(file_path):\n",
    "        process_wdbc_dataset(file_path, name)\n",
    "    else:\n",
    "        print(f\"File '{file_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe956391",
   "metadata": {},
   "source": [
    "# Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7110719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing: original\n",
      "========================================\n",
      "Top features with highest correlation to Target:\n",
      "Feature_2: 0.4666\n",
      "Feature_6: 0.2927\n",
      "Feature_8: 0.2384\n",
      "Feature_1: 0.2219\n",
      "\n",
      "========================================\n",
      "Processing: noise\n",
      "========================================\n",
      "Top features with highest correlation to Target:\n",
      "Feature_2: 0.4008\n",
      "Feature_6: 0.2428\n",
      "Feature_8: 0.2266\n",
      "Feature_1: 0.2061\n",
      "\n",
      "========================================\n",
      "Processing: outlier\n",
      "========================================\n",
      "Top features with highest correlation to Target:\n",
      "Feature_2: 0.3548\n",
      "Feature_6: 0.2152\n",
      "Feature_8: 0.2026\n",
      "Feature_1: 0.1668\n",
      "\n",
      "========================================\n",
      "Processing: noise_outlier\n",
      "========================================\n",
      "Top features with highest correlation to Target:\n",
      "Feature_2: 0.2522\n",
      "Feature_6: 0.1808\n",
      "Feature_8: 0.1742\n",
      "Feature_1: 0.1572\n"
     ]
    }
   ],
   "source": [
    "def process_diabetes_dataset(file_path, dataset_name):\n",
    "    print(f\"\\n{'='*40}\\nProcessing: {dataset_name}\\n{'='*40}\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df = df.rename(columns={df.columns[-1]: 'Target'})\n",
    "    df['Target'] = np.where(df['Target'] == 0, -1, 1)\n",
    "\n",
    "    new_columns = {col: f'Feature_{i+1}' for i, col in enumerate(df.columns) if col != 'Target'}\n",
    "    new_columns['Target'] = 'Target'\n",
    "    df = df.rename(columns=new_columns)\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='YlGnBu')\n",
    "    plt.title(f'Correlation Matrix - {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_dir = os.path.join(PATH, 'src', 'experiment', 'results', 'correlation_matrix', 'diabetes')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(os.path.join(save_dir, f'correlation_heatmap_diabetes_{dataset_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    target_correlations = corr_matrix['Target'].abs().sort_values(ascending=False)\n",
    "\n",
    "    top_features = target_correlations[1:int(df.shape[1]/2)+1]\n",
    "\n",
    "    print(\"Top features with highest correlation to Target:\")\n",
    "    for feature, corr in top_features.items():\n",
    "        print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Danh sách file\n",
    "datasets = {\n",
    "    \"original\": os.path.join(FILE_PATH, \"diabetes.csv\"),\n",
    "    \"noise\": os.path.join(FILE_PATH, \"diabetes_noise_label_feature.csv\"),\n",
    "    \"outlier\": os.path.join(FILE_PATH, \"diabetes_outlier.csv\"),\n",
    "    \"noise_outlier\": os.path.join(FILE_PATH, \"diabetes_both_noise_outlier.csv\")\n",
    "}\n",
    "\n",
    "# Gọi hàm xử lý từng file\n",
    "for name, path in datasets.items():\n",
    "    if os.path.exists(path):\n",
    "        process_diabetes_dataset(path, name)\n",
    "    else:\n",
    "        print(f\"File '{path}' does not exist.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72652f",
   "metadata": {},
   "source": [
    "# Cleveland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edac90e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing: original\n",
      "========================================\n",
      "Top features with highest correlation to Target:\n",
      "Feature_13: 0.5159\n",
      "Feature_12: 0.4600\n",
      "Feature_9: 0.4319\n",
      "Feature_10: 0.4245\n",
      "Feature_8: 0.4172\n",
      "Feature_3: 0.4144\n",
      "Feature_11: 0.3392\n",
      "\n",
      "========================================\n",
      "Processing: noise\n",
      "========================================\n",
      "Top features with highest correlation to Target:\n",
      "Feature_10: 0.3825\n",
      "Feature_13: 0.3818\n",
      "Feature_12: 0.3262\n",
      "Feature_3: 0.2461\n",
      "Feature_9: 0.2337\n",
      "Feature_11: 0.2201\n",
      "Feature_8: 0.1744\n",
      "\n",
      "========================================\n",
      "Processing: outlier\n",
      "========================================\n",
      "Top features with highest correlation to Target:\n",
      "Feature_13: 0.4207\n",
      "Feature_9: 0.3504\n",
      "Feature_3: 0.3316\n",
      "Feature_8: 0.3281\n",
      "Feature_10: 0.2848\n",
      "Feature_12: 0.2572\n",
      "Feature_11: 0.2566\n",
      "\n",
      "========================================\n",
      "Processing: noise_outlier\n",
      "========================================\n",
      "Top features with highest correlation to Target:\n",
      "Feature_10: 0.3825\n",
      "Feature_13: 0.3818\n",
      "Feature_12: 0.3262\n",
      "Feature_3: 0.2461\n",
      "Feature_9: 0.2337\n",
      "Feature_11: 0.2201\n",
      "Feature_8: 0.1744\n"
     ]
    }
   ],
   "source": [
    "def process_heart_dataset(file_path, dataset_name):\n",
    "    print(f\"\\n{'='*40}\\nProcessing: {dataset_name}\\n{'='*40}\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    if 'target' in df.columns:\n",
    "        df = df.rename(columns={'target': 'Target'})\n",
    "    elif 'Target' not in df.columns:\n",
    "        df.columns = list(df.columns[:-1]) + ['Target']\n",
    "\n",
    "    df['Target'] = np.where(df['Target'] == 0, -1, 1)\n",
    "\n",
    "    new_columns = {col: f'Feature_{i+1}' for i, col in enumerate(df.columns) if col != 'Target'}\n",
    "    new_columns['Target'] = 'Target'\n",
    "    df = df.rename(columns=new_columns)\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='YlGnBu', cbar=True)\n",
    "    plt.title(f'Correlation Matrix - {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    save_dir = os.path.join(PATH, 'src', 'experiment', 'results', 'correlation_matrix', 'cleveland')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(os.path.join(save_dir, f'correlation_heatmap_cleveland_{dataset_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    target_correlations = corr_matrix['Target'].abs().sort_values(ascending=False)\n",
    "\n",
    "    top_features = target_correlations[1:int(df.shape[1]/2)+1]\n",
    "\n",
    "    print(\"Top features with highest correlation to Target:\")\n",
    "    for feature, corr in top_features.items():\n",
    "        print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Danh sách các file\n",
    "datasets = {\n",
    "    \"original\": os.path.join(FILE_PATH, \"Heart_disease_cleveland_new.csv\"),\n",
    "    \"noise\": os.path.join(FILE_PATH, \"clevaland_noise_label_feature.csv\"),\n",
    "    \"outlier\": os.path.join(FILE_PATH, \"clevaland_outlier.csv\"),\n",
    "    \"noise_outlier\": os.path.join(FILE_PATH, \"clevaland_noise_label_feature.csv\")\n",
    "}\n",
    "\n",
    "# Gọi hàm xử lý\n",
    "for name, path in datasets.items():\n",
    "    if os.path.exists(path):\n",
    "        process_heart_dataset(path, name)\n",
    "    else:\n",
    "        print(f\" File '{path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd8668",
   "metadata": {},
   "source": [
    "# Sonar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c30ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing: original\n",
      "========================================\n",
      "Top 10 features with highest correlation to Target:\n",
      "Feature_11: 0.4329\n",
      "Feature_12: 0.3922\n",
      "Feature_49: 0.3513\n",
      "Feature_10: 0.3411\n",
      "Feature_45: 0.3394\n",
      "Feature_48: 0.3293\n",
      "Feature_9: 0.3214\n",
      "Feature_13: 0.3128\n",
      "Feature_46: 0.3056\n",
      "Feature_47: 0.3017\n",
      "\n",
      "========================================\n",
      "Processing: noise\n",
      "========================================\n",
      "Top 10 features with highest correlation to Target:\n",
      "Feature_9: 0.2562\n",
      "Feature_36: 0.2120\n",
      "Feature_37: 0.2034\n",
      "Feature_46: 0.1970\n",
      "Feature_12: 0.1865\n",
      "Feature_13: 0.1800\n",
      "Feature_20: 0.1754\n",
      "Feature_21: 0.1746\n",
      "Feature_11: 0.1709\n",
      "Feature_43: 0.1669\n",
      "\n",
      "========================================\n",
      "Processing: outlier\n",
      "========================================\n",
      "Top 10 features with highest correlation to Target:\n",
      "Feature_12: 0.3399\n",
      "Feature_11: 0.3271\n",
      "Feature_49: 0.2971\n",
      "Feature_45: 0.2916\n",
      "Feature_48: 0.2755\n",
      "Feature_10: 0.2588\n",
      "Feature_46: 0.2530\n",
      "Feature_47: 0.2526\n",
      "Feature_9: 0.2498\n",
      "Feature_13: 0.2431\n",
      "\n",
      "========================================\n",
      "Processing: noise_outlier\n",
      "========================================\n",
      "Top 10 features with highest correlation to Target:\n",
      "Feature_48: 0.2145\n",
      "Feature_9: 0.1958\n",
      "Feature_12: 0.1892\n",
      "Feature_1: 0.1802\n",
      "Feature_39: 0.1768\n",
      "Feature_45: 0.1672\n",
      "Feature_46: 0.1573\n",
      "Feature_2: 0.1471\n",
      "Feature_10: 0.1380\n",
      "Feature_60: 0.1290\n"
     ]
    }
   ],
   "source": [
    "def process_sonar_dataset(file_path, dataset_name):\n",
    "    print(f\"\\n{'='*40}\\nProcessing: {dataset_name}\\n{'='*40}\")\n",
    "\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "    df = df.rename(columns={df.columns[-1]: 'Target'})\n",
    "    df['Target'] = np.where(df['Target'] == 'M', 1, -1)\n",
    "\n",
    "    new_columns = {col: f'Feature_{i+1}' for i, col in enumerate(df.columns) if col != 'Target'}\n",
    "    new_columns['Target'] = 'Target'\n",
    "    df = df.rename(columns=new_columns)\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    plt.figure(figsize=(25, 20))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='YlGnBu')\n",
    "    plt.title(f'Correlation Matrix - {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    save_dir = os.path.join(PATH, 'src', 'experiment', 'results', 'correlation_matrix', 'sonar')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(os.path.join(save_dir, f'correlation_heatmap_sonar_{dataset_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    target_correlations = corr_matrix['Target'].abs().sort_values(ascending=False)\n",
    "\n",
    "    top_10_features = target_correlations[1:11]\n",
    "    print(\"Top 10 features with highest correlation to Target:\")\n",
    "    for feature, corr in top_10_features.items():\n",
    "        print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Danh sách các file sonar\n",
    "datasets = {\n",
    "    \"original\": os.path.join(FILE_PATH, \"sonar.txt\"),\n",
    "    \"noise\": os.path.join(FILE_PATH, \"sonar_noise_label_feature.txt\"),\n",
    "    \"outlier\": os.path.join(FILE_PATH, \"sonar_outlier.txt\"),\n",
    "    \"noise_outlier\": os.path.join(FILE_PATH, \"sonar_both_noise_outlier.txt\")\n",
    "}\n",
    "\n",
    "for name, path in datasets.items():\n",
    "    if os.path.exists(path):\n",
    "        process_sonar_dataset(path, name)\n",
    "    else:\n",
    "        print(f\"File '{path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1232e7",
   "metadata": {},
   "source": [
    "# Ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a65d4337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing: original\n",
      "========================================\n",
      "Top 10 features with highest correlation to Target:\n",
      "Feature_3: 0.5191\n",
      "Feature_5: 0.5165\n",
      "Feature_1: 0.4656\n",
      "Feature_7: 0.4504\n",
      "Feature_9: 0.2949\n",
      "Feature_31: 0.2944\n",
      "Feature_33: 0.2612\n",
      "Feature_29: 0.2500\n",
      "Feature_21: 0.2196\n",
      "Feature_8: 0.2075\n",
      "\n",
      "========================================\n",
      "Processing: noise\n",
      "========================================\n",
      "Top 10 features with highest correlation to Target:\n",
      "Feature_3: 0.4398\n",
      "Feature_5: 0.4297\n",
      "Feature_7: 0.4065\n",
      "Feature_1: 0.3733\n",
      "Feature_9: 0.2938\n",
      "Feature_31: 0.2580\n",
      "Feature_21: 0.2447\n",
      "Feature_33: 0.2404\n",
      "Feature_15: 0.2249\n",
      "Feature_29: 0.2131\n",
      "\n",
      "========================================\n",
      "Processing: outlier\n",
      "========================================\n",
      "Top 10 features with highest correlation to Target:\n",
      "Feature_5: 0.4727\n",
      "Feature_3: 0.4461\n",
      "Feature_7: 0.4408\n",
      "Feature_1: 0.4202\n",
      "Feature_9: 0.3367\n",
      "Feature_31: 0.2715\n",
      "Feature_33: 0.2613\n",
      "Feature_15: 0.2586\n",
      "Feature_13: 0.2408\n",
      "Feature_11: 0.2343\n",
      "\n",
      "========================================\n",
      "Processing: noise_outlier\n",
      "========================================\n",
      "Top 10 features with highest correlation to Target:\n",
      "Feature_5: 0.3966\n",
      "Feature_3: 0.3875\n",
      "Feature_7: 0.3842\n",
      "Feature_1: 0.2934\n",
      "Feature_9: 0.2599\n",
      "Feature_33: 0.2576\n",
      "Feature_31: 0.2565\n",
      "Feature_15: 0.2434\n",
      "Feature_11: 0.2160\n",
      "Feature_13: 0.1990\n"
     ]
    }
   ],
   "source": [
    "def process_ionosphere_dataset(file_path, dataset_name):\n",
    "    print(f\"\\n{'='*40}\\nProcessing: {dataset_name}\\n{'='*40}\")\n",
    "\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "    df = df.rename(columns={df.columns[-1]: 'Target'})\n",
    "    df['Target'] = np.where(df['Target'] == 'g', 1, -1)\n",
    "\n",
    "    new_columns = {col: f'Feature_{col + 1}' for col in df.columns if col != 'Target'}\n",
    "    new_columns['Target'] = 'Target'\n",
    "    df = df.rename(columns=new_columns)\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='YlGnBu')\n",
    "    plt.title(f'Correlation Matrix - {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    save_dir = os.path.join(PATH, 'src', 'experiment', 'results', 'correlation_matrix', 'ionosphere')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(os.path.join(save_dir, f'correlation_heatmap_ionosphere_{dataset_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    target_corr = corr_matrix['Target'].abs().sort_values(ascending=False)\n",
    "\n",
    "    top_10_features = target_corr[1:11]\n",
    "\n",
    "\n",
    "    print(\"Top 10 features with highest correlation to Target:\")\n",
    "    for f, c in top_10_features.items():\n",
    "        print(f\"{f}: {c:.4f}\")\n",
    "\n",
    "# Danh sách các file ionosphere\n",
    "datasets = {\n",
    "    \"original\": os.path.join(FILE_PATH, \"ionosphere.data\"),\n",
    "    \"noise\": os.path.join(FILE_PATH, \"ionosphere_noise_label_feature.txt\"),\n",
    "    \"outlier\": os.path.join(FILE_PATH, \"ionosphere_outlier.txt\"),\n",
    "    \"noise_outlier\": os.path.join(FILE_PATH, \"ionosphere_both_noise_outlier.txt\")\n",
    "}\n",
    "\n",
    "\n",
    "for name, path in datasets.items():\n",
    "    if os.path.exists(path):\n",
    "        process_ionosphere_dataset(path, name)\n",
    "    else:\n",
    "        print(f\"File '{path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1df9c7",
   "metadata": {},
   "source": [
    "# Colon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb694ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing: original\n",
      "========================================\n",
      "Dataset shape: (61, 2001)\n",
      "Computing correlation matrix...\n",
      "Creating correlation heatmap for top 20 features...\n",
      "Top 10 features:\n",
      "Feature_249: 0.6296\n",
      "Feature_493: 0.5978\n",
      "Feature_765: 0.5966\n",
      "Feature_1423: 0.5879\n",
      "Feature_245: 0.5817\n",
      "Feature_267: 0.5731\n",
      "Feature_377: 0.5457\n",
      "Feature_822: 0.5388\n",
      "Feature_1892: 0.5017\n",
      "Feature_1772: 0.5000\n",
      "\n",
      "========================================\n",
      "Processing: noise\n",
      "========================================\n",
      "Dataset shape: (61, 2001)\n",
      "Computing correlation matrix...\n",
      "Creating correlation heatmap for top 20 features...\n",
      "Top 10 features:\n",
      "Feature_249: 0.5349\n",
      "Feature_1423: 0.4344\n",
      "Feature_780: 0.4335\n",
      "Feature_765: 0.4306\n",
      "Feature_1771: 0.4173\n",
      "Feature_822: 0.4027\n",
      "Feature_1346: 0.4015\n",
      "Feature_897: 0.4005\n",
      "Feature_267: 0.3981\n",
      "Feature_1060: 0.3961\n",
      "\n",
      "========================================\n",
      "Processing: outlier\n",
      "========================================\n",
      "Dataset shape: (61, 2001)\n",
      "Computing correlation matrix...\n",
      "Creating correlation heatmap for top 20 features...\n",
      "Top 10 features:\n",
      "Feature_1924: 0.4214\n",
      "Feature_927: 0.3831\n",
      "Feature_18: 0.3726\n",
      "Feature_792: 0.3684\n",
      "Feature_377: 0.3683\n",
      "Feature_1827: 0.3668\n",
      "Feature_209: 0.3656\n",
      "Feature_1203: 0.3644\n",
      "Feature_99: 0.3604\n",
      "Feature_624: 0.3580\n",
      "\n",
      "========================================\n",
      "Processing: noise_outlier\n",
      "========================================\n",
      "Dataset shape: (61, 2001)\n",
      "Computing correlation matrix...\n",
      "Creating correlation heatmap for top 20 features...\n",
      "Top 10 features:\n",
      "Feature_56: 0.4354\n",
      "Feature_1667: 0.4352\n",
      "Feature_1924: 0.4320\n",
      "Feature_449: 0.4287\n",
      "Feature_104: 0.4167\n",
      "Feature_14: 0.4162\n",
      "Feature_875: 0.4147\n",
      "Feature_514: 0.4065\n",
      "Feature_1750: 0.3984\n",
      "Feature_801: 0.3919\n"
     ]
    }
   ],
   "source": [
    "def process_colon_dataset(file_path, dataset_name):\n",
    "    print(f\"\\n{'='*40}\\nProcessing: {dataset_name}\\n{'='*40}\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "    df = df.rename(columns={df.columns[-1]: 'Target'})\n",
    "    df['Target'] = np.where(df['Target'] == 2, 1, -1)  \n",
    "\n",
    "    new_columns = {col: f'Feature_{i+1}' for i, col in enumerate(df.columns) if col != 'Target'}\n",
    "    new_columns['Target'] = 'Target'\n",
    "    df = df.rename(columns=new_columns)\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "\n",
    "    print(\"Computing correlation matrix...\")\n",
    "    corr_matrix = df.corr()\n",
    "    target_correlations = corr_matrix['Target'].abs().sort_values(ascending=False)\n",
    "    top_features = target_correlations[1:21]  # Top 20\n",
    "\n",
    "    print(\"Creating correlation heatmap for top 20 features...\")\n",
    "    top_feature_names = ['Target'] + list(top_features.index[:20])\n",
    "    small_corr = corr_matrix.loc[top_feature_names, top_feature_names]\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    sns.heatmap(small_corr, \n",
    "                annot=True,  \n",
    "                fmt='.2f', \n",
    "                cmap='YlGnBu',\n",
    "                square=True)\n",
    "    plt.title(f'Top 20 Features Correlation Matrix - {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_dir = os.path.join(PATH, 'src', 'experiment', 'results', 'correlation_matrix', 'colon')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(os.path.join(save_dir, f'correlation_heatmap_colon_{dataset_name}.png'), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Top 10 features:\")\n",
    "    for feature, corr in top_features.head(10).items():\n",
    "        print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "datasets = {\n",
    "    \"original\": os.path.join(FILE_PATH, \"colon.csv\"),\n",
    "    \"noise\": os.path.join(FILE_PATH, \"colon_noise_label_feature.csv\"),\n",
    "    \"outlier\": os.path.join(FILE_PATH, \"colon_outlier.csv\"),\n",
    "    \"noise_outlier\": os.path.join(FILE_PATH, \"colon_both_noise_outlier.csv\")\n",
    "}\n",
    "\n",
    "for name, path in datasets.items():\n",
    "    if os.path.exists(path):\n",
    "        process_colon_dataset(path, name)\n",
    "    else:\n",
    "        print(f\"File '{path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4cf303",
   "metadata": {},
   "source": [
    "# Top common features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "553ab9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wdbc...\n",
      "Updated wdbc with overlap analysis\n",
      "Processing diabetes...\n",
      "Updated diabetes with overlap analysis\n",
      "Processing cleveland...\n",
      "Updated cleveland with overlap analysis\n",
      "Processing sonar...\n",
      "Updated sonar with overlap analysis\n",
      "Processing ionosphere...\n",
      "Updated ionosphere with overlap analysis\n",
      "Processing wdbc...\n",
      "Updated wdbc with overlap analysis\n",
      "Processing diabetes...\n",
      "Updated diabetes with overlap analysis\n",
      "Processing cleveland...\n",
      "Updated cleveland with overlap analysis\n",
      "Processing sonar...\n",
      "Updated sonar with overlap analysis\n",
      "Processing ionosphere...\n",
      "Updated ionosphere with overlap analysis\n",
      "Processing wdbc...\n",
      "Updated wdbc with overlap analysis\n",
      "Processing diabetes...\n",
      "Updated diabetes with overlap analysis\n",
      "Processing cleveland...\n",
      "Updated cleveland with overlap analysis\n",
      "Processing sonar...\n",
      "Updated sonar with overlap analysis\n",
      "Processing ionosphere...\n",
      "Updated ionosphere with overlap analysis\n",
      "Processing wdbc...\n",
      "Updated wdbc with overlap analysis\n",
      "Processing diabetes...\n",
      "Updated diabetes with overlap analysis\n",
      "Processing cleveland...\n",
      "Updated cleveland with overlap analysis\n",
      "Processing sonar...\n",
      "Updated sonar with overlap analysis\n",
      "Processing ionosphere...\n",
      "Updated ionosphere with overlap analysis\n",
      "Processing wdbc...\n",
      "Updated wdbc with overlap analysis\n",
      "Processing diabetes...\n",
      "Updated diabetes with overlap analysis\n",
      "Processing cleveland...\n",
      "Updated cleveland with overlap analysis\n",
      "Processing sonar...\n",
      "Updated sonar with overlap analysis\n",
      "Processing ionosphere...\n",
      "Updated ionosphere with overlap analysis\n",
      "Feature overlap analysis completed for all datasets.\n"
     ]
    }
   ],
   "source": [
    "def extract_features_from_string(feature_string):\n",
    "    \"\"\"Extract feature numbers from comma-separated string\"\"\"\n",
    "    if pd.isna(feature_string) or feature_string == '':\n",
    "        return []\n",
    "    return [int(x.strip()) for x in str(feature_string).split(',')]\n",
    "\n",
    "def get_top_correlation_features(dataset_name, dataset_type):\n",
    "    \"\"\"Get top correlation features from your correlation analysis\"\"\"\n",
    "    correlation_results = {\n",
    "        'wdbc': {\n",
    "            'original': [28,23,8,21,3,24,1,4,7,27],  \n",
    "            'noise': [24,3,4,21,23,14,1,13,27,11],\n",
    "            'outlier': [28, 8, 27, 23, 21, 7, 3, 1, 26, 6],\n",
    "            'both': [23, 4, 21, 3, 24, 1, 27, 26, 22, 13]\n",
    "        },\n",
    "        'diabetes': {\n",
    "            'original': [2,6,8,1],\n",
    "            'noise': [2, 6, 8, 1],\n",
    "            'outlier': [2, 6, 8, 1],\n",
    "            'both': [2, 6, 8, 1]\n",
    "        },\n",
    "        'cleveland': {\n",
    "            'original': [13,12,9,10,8,3,11],\n",
    "            'noise': [10,13,12,3,9,11,8],\n",
    "            'outlier': [13, 9, 3, 8, 10, 12, 11],\n",
    "            'both': [10, 13, 12, 3, 9, 11, 8]\n",
    "        },\n",
    "        'sonar':{\n",
    "            'original': [11,12,49,10,45,48,9,13,46,47],\n",
    "            'noise': [9, 36, 37, 46, 12, 13, 20, 21, 11, 43],\n",
    "            'outlier': [12, 11, 49, 45, 48, 10, 46, 47, 9, 13],\n",
    "            'both': [48, 9, 12, 1, 39, 45, 46, 2, 10, 60]\n",
    "        },\n",
    "        'ionosphere':{\n",
    "            'original': [3,5,1,7,9,31,33,29,21,8],\n",
    "            'noise': [3, 5, 7, 1, 9, 31, 21, 33, 15, 29],\n",
    "            'outlier': [5, 3, 7, 1, 9, 31, 33, 15, 13, 11],\n",
    "            'both': [5, 3, 7, 1, 9, 33, 31, 15, 11, 13]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return correlation_results.get(dataset_name, {}).get(dataset_type, [])\n",
    "\n",
    "def analyze_feature_overlap(dataset_name, dataset_type='original'):\n",
    "    \"\"\"Analyze feature overlap for a specific dataset and type\"\"\"\n",
    "    \n",
    "    datasets = ['wdbc', 'diabetes', 'cleveland', 'sonar', 'ionosphere']\n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        results_file = os.path.join(PATH, 'src', 'experiment', 'results', f'experiment_results_{dataset_name}.xlsx')\n",
    "        \n",
    "        if not os.path.exists(results_file):\n",
    "            print(f\"Results file not found: {results_file}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing {dataset_name}...\")\n",
    "        \n",
    "        # Read experiment results\n",
    "        df = pd.read_excel(results_file)\n",
    "        \n",
    "        # Map dataset types\n",
    "        type_mapping = {\n",
    "            'Not noise': 'original',\n",
    "            'Noise': 'noise',\n",
    "            'Outlier': 'outlier',\n",
    "            'Noise + Outlier': 'both'\n",
    "        }\n",
    "        \n",
    "        # Initialize new columns\n",
    "        df['Top 10 Correlation Features'] = ''\n",
    "        df['Overlap Count'] = 0\n",
    "        df['Overlap Ratio'] = 0.0\n",
    "        df['Common Features'] = ''\n",
    "        \n",
    "        # Process each row\n",
    "        for idx, row in df.iterrows():\n",
    "            dataset_type = type_mapping.get(row['Type of dataset'], 'original')\n",
    "            \n",
    "            # Get top correlation features\n",
    "            top_corr_features = get_top_correlation_features(dataset_name, dataset_type)\n",
    "            df.at[idx, 'Top 10 Correlation Features'] = ', '.join(map(str, top_corr_features))\n",
    "            \n",
    "            # Extract selected features\n",
    "            feature_string = ''\n",
    "            if 'BestFold Features selected' in row and pd.notna(row['BestFold Features selected']):\n",
    "                feature_string = row['BestFold Features selected']\n",
    "            elif 'Features selected' in row and pd.notna(row['Features selected']):\n",
    "                feature_string = row['Features selected']\n",
    "            \n",
    "            if feature_string:\n",
    "                selected_features = extract_features_from_string(feature_string)\n",
    "                \n",
    "                if selected_features and top_corr_features:\n",
    "                    # Calculate overlap\n",
    "                    common_features = set(selected_features).intersection(set(top_corr_features))\n",
    "                    overlap_count = len(common_features)\n",
    "                    overlap_ratio = overlap_count / len(selected_features)\n",
    "                    \n",
    "                    df.at[idx, 'Overlap Count'] = overlap_count\n",
    "                    df.at[idx, 'Overlap Ratio'] = round(overlap_ratio, 4)\n",
    "                    df.at[idx, 'Common Features'] = ', '.join(map(str, sorted(common_features)))\n",
    "        \n",
    "        # Save updated file\n",
    "        df.to_excel(results_file, index=False)\n",
    "        print(f\"Updated {dataset_name} with overlap analysis\")\n",
    "if __name__ == \"__main__\":\n",
    "    datasets = ['wdbc', 'diabetes', 'cleveland', 'sonar', 'ionosphere']\n",
    "    for dataset in datasets:\n",
    "        analyze_feature_overlap(dataset)\n",
    "    print(\"Feature overlap analysis completed for all datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8bf0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
